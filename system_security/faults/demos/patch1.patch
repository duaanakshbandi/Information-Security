---
 mm/ksm.c | 39 ++++++++++++++++++++++++++++++---------
 1 file changed, 30 insertions(+), 9 deletions(-)

diff --git a/mm/ksm.c b/mm/ksm.c
index 74b4d4747482..dbd9e01c8a1f 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -1221,11 +1221,13 @@ static int replace_page(struct vm_area_struct *vma, struct page *page,
  * @page: the PageAnon page that we want to replace with kpage
  * @kpage: the PageKsm page that we want to map instead of page,
  *         or NULL the first time when we want to use page as kpage.
+ * @wrprotect_only: writeprotect, but don't create the PageKSM.
  *
  * This function returns 0 if the pages were merged, -EFAULT otherwise.
  */
 static int try_to_merge_one_page(struct vm_area_struct *vma,
-                struct page *page, struct page *kpage)
+                struct page *page, struct page *kpage,
+                bool wrprotect_only)
 {
    pte_t orig_pte = __pte(0);
    int err = -EFAULT;
@@ -1259,12 +1261,16 @@ static int try_to_merge_one_page(struct vm_area_struct *vma,
     */
    if (write_protect_page(vma, page, &orig_pte) == 0) {
        if (!kpage) {
-           /*
-            * While we hold page lock, upgrade page from
-            * PageAnon+anon_vma to PageKsm+NULL stable_node:
-            * stable_tree_insert() will update stable_node.
-            */
-           set_page_stable_node(page, NULL);
+           if (!wrprotect_only) {
+               /*
+                * While we hold page lock, upgrade
+                * page from PageAnon+anon_vma to
+                * PageKsm+NULL stable_node:
+                * stable_tree_insert() will update
+                * stable_node.
+                */
+               set_page_stable_node(page, NULL);
+           }
            mark_page_accessed(page);
            /*
             * Page reclaim just frees a clean page with no dirty
@@ -1293,6 +1299,19 @@ static int try_to_merge_one_page(struct vm_area_struct *vma,
    return err;
 }
 
+static void try_to_write_protect_page(struct rmap_item *rmap_item,
+                     struct page *page)
+{
+   struct mm_struct *mm = rmap_item->mm;
+   struct vm_area_struct *vma;
+
+   mmap_read_lock(mm);
+   vma = find_mergeable_vma(mm, rmap_item->address);
+   if (vma)
+       try_to_merge_one_page(vma, page, NULL, true);
+   mmap_read_unlock(mm);
+}
+
 /*
  * try_to_merge_with_ksm_page - like try_to_merge_two_pages,
  * but no new kernel page is allocated: kpage must already be a ksm page.
@@ -1311,7 +1330,7 @@ static int try_to_merge_with_ksm_page(struct rmap_item *rmap_item,
    if (!vma)
        goto out;
 
-   err = try_to_merge_one_page(vma, page, kpage);
+   err = try_to_merge_one_page(vma, page, kpage, false);
    if (err)
        goto out;
 
@@ -2138,7 +2157,7 @@ static void cmp_and_merge_page(struct page *page, struct rmap_item *rmap_item)
        vma = find_mergeable_vma(mm, rmap_item->address);
        if (vma) {
            err = try_to_merge_one_page(vma, page,
-                   ZERO_PAGE(rmap_item->address));
+                   ZERO_PAGE(rmap_item->address), false);
        } else {
            /*
             * If the vma is out of date, we do not need to
@@ -2214,6 +2233,8 @@ static void cmp_and_merge_page(struct page *page, struct rmap_item *rmap_item)
            split_huge_page(page);
            unlock_page(page);
        }
+   } else if (must_write_protect()) {
+       try_to_write_protect_page(rmap_item, page);
    }
 }
 